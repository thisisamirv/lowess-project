% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fastlowess_streaming.R
\name{fastlowess_streaming}
\alias{fastlowess_streaming}
\title{Streaming LOWESS for Large Datasets}
\usage{
fastlowess_streaming(
  x,
  y,
  fraction = 0.3,
  chunk_size = 5000L,
  overlap = NULL,
  iterations = 3L,
  delta = NULL,
  weight_function = "tricube",
  robustness_method = "bisquare",
  scaling_method = "mad",
  boundary_policy = "extend",
  auto_converge = NULL,
  return_diagnostics = FALSE,
  return_robustness_weights = FALSE,
  parallel = TRUE
)
}
\arguments{
\item{x}{Numeric vector of independent variable values.}

\item{y}{Numeric vector of dependent variable values (same length as x).}

\item{fraction}{Smoothing fraction (default: 0.3). Lower values (0.1-0.3)
are typically recommended for streaming to maintain good local precision
within small chunks.}

\item{chunk_size}{Number of points to process in each chunk (default: 5000).
Larger chunks improve smoothness but increase memory usage.}

\item{overlap}{Number of points to overlap between chunks (default: 10
percent of chunk_size). Overlap ensures smooth transitions and consistent
fits across chunk boundaries. Overlapping values are merged (typically
averaged).}

\item{iterations}{Number of robustness iterations (default: 3).
\itemize{
  \item **0**: Fastest; standard least-squares within chunks.
  \item **1-2**: Recommended for light to moderate outliers.
  \item **3**: Default; high resistance to noise.
}}

\item{delta}{Interpolation optimization threshold. NULL (default)
auto-calculates. Set to 0 to disable interpolation.}

\item{weight_function}{Kernel function for distance weighting. Options:
"tricube" (default), "epanechnikov", "gaussian", "uniform", "biweight",
"triangle", "cosine".}

\item{robustness_method}{Method for computing robustness weights. Options:
"bisquare" (default), "huber", "talwar".}

\item{scaling_method}{Scaling method for robustness weight calculation.
Options: "mad" (default), "mar" (Median Absolute Residual).}

\item{boundary_policy}{Handling of edge effects. Options: "extend" (default),
"reflect", "zero", "noboundary".}

\item{auto_converge}{Tolerance for automatic convergence. NULL (default)
disables.}

\item{return_diagnostics}{Logical, whether to compute cumulative fit
quality metrics across all chunks. Default: FALSE.}

\item{return_robustness_weights}{Logical, whether to include robustness
weights in output. Default: FALSE.}

\item{parallel}{Logical, whether to enable parallel chunk processing
(default: TRUE). Chunks are processed in parallel via Rayon, significantly
improving throughput for large datasets.}
}
\value{
A list containing:
\itemize{
  \item \code{x}: Sorted independent variable values.
  \item \code{y}: Smoothed dependent variable values.
  \item \code{fraction_used}: The fraction used for smoothing.
  \item \code{diagnostics}: Cumulative metrics (RMSE, etc.) (if requested).
  \item \code{robustness_weights}: Weights for each point (if requested).
}
}
\description{
Perform LOWESS smoothing using a streaming/chunked approach for large
datasets. Processes data in chunks to maintain constant memory usage,
suitable for datasets too large to fit in memory.

## When to use streaming smoothing:
\itemize{
  \item Dataset size exceeds available system memory (>100K points).
  \item Processing large data files in a pipeline.
  \item Memory usage must be strictly bounded.
}
}
\examples{
# Process a large dataset in chunks
n <- 20000
x <- seq(0, 100, length.out = n)
y <- sin(x / 10) + rnorm(n, sd = 0.5)

# streaming approach maintains low memory footprint
result <- fastlowess_streaming(x, y, chunk_size = 5000L, overlap = 500L)

plot(x, y, pch = ".")
lines(result$x, result$y, col = "red", lwd = 2)

}
\seealso{
\code{\link{fastlowess}}, \code{\link{fastlowess_online}}

Other fastlowess: 
\code{\link{fastlowess}()},
\code{\link{fastlowess_online}()}
}
\concept{fastlowess}
